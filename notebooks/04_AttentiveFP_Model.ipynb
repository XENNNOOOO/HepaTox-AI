{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "04385164",
   "metadata": {},
   "source": [
    "## Notebook 4: State-of-the-Art Model (AttentiveFP)\n",
    "\n",
    "Implement, train, and evaluate an AttentiveFP model, a state-of-the-art GNN architecture designed for molecular property prediction. Our aim is to surpass the RandomForest baseline and achieve an ROC AUC score greater than 0.81."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a451f036",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "92639985",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import ast\n",
    "import torch\n",
    "import torch.nn.functional as F\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "# RDKit for chemoinformatics\n",
    "from rdkit import Chem\n",
    "\n",
    "# PyTorch Geometric\n",
    "from torch_geometric.data import Data, DataLoader\n",
    "from torch_geometric.nn.models import AttentiveFP # Import the AttentiveFP model\n",
    "\n",
    "# Scikit-learn for evaluation\n",
    "from sklearn.model_selection import train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "\n",
    "print(\"Libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "08e5677e",
   "metadata": {},
   "source": [
    "### Load and Prepare Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0d9d600",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "try:\n",
    "    df = pd.read_csv('data/processed/dili_data_clean.csv')\n",
    "    df.dropna(subset=['fingerprint', 'smiles'], inplace=True)\n",
    "    print(\"Processed data loaded successfully.\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: dili_data_clean.csv not found.\")\n",
    "\n",
    "# Graph Conversion Functions \n",
    "def get_atom_features(atom):\n",
    "    features = []\n",
    "    features.append(atom.GetAtomicNum())\n",
    "    features.append(atom.GetDegree())\n",
    "    features.append(atom.GetFormalCharge())\n",
    "    features.append(int(atom.GetHybridization()))\n",
    "    features.append(atom.GetIsAromatic())\n",
    "    return features\n",
    "\n",
    "def get_bond_features(bond):\n",
    "    bond_type = bond.GetBondType()\n",
    "    return [\n",
    "        bond_type == Chem.rdchem.BondType.SINGLE,\n",
    "        bond_type == Chem.rdchem.BondType.DOUBLE,\n",
    "        bond_type == Chem.rdchem.BondType.TRIPLE,\n",
    "        bond_type == Chem.rdchem.BondType.AROMATIC,\n",
    "        bond.GetIsConjugated(),\n",
    "        bond.IsInRing(),\n",
    "    ]\n",
    "\n",
    "def smiles_to_graph(smiles):\n",
    "    mol = Chem.MolFromSmiles(smiles)\n",
    "    if mol is None: return None\n",
    "    \n",
    "    atom_features = [get_atom_features(atom) for atom in mol.GetAtoms()]\n",
    "    x = torch.tensor(atom_features, dtype=torch.float)\n",
    "\n",
    "    edge_indices, edge_attrs = [], []\n",
    "    for bond in mol.GetBonds():\n",
    "        i, j = bond.GetBeginAtomIdx(), bond.GetEndAtomIdx()\n",
    "        bond_feats = get_bond_features(bond)\n",
    "        edge_indices.extend([(i, j), (j, i)])\n",
    "        edge_attrs.extend([bond_feats, bond_feats])\n",
    "\n",
    "    edge_index = torch.tensor(edge_indices, dtype=torch.long).t().contiguous()\n",
    "    edge_attr = torch.tensor(edge_attrs, dtype=torch.float)\n",
    "    \n",
    "    return Data(x=x, edge_index=edge_index, edge_attr=edge_attr)\n",
    "\n",
    "# Create Graph Dataset\n",
    "print(\"Converting SMILES to graph objects...\")\n",
    "data_list = [smiles_to_graph(s) for s in tqdm(df['smiles'])]\n",
    "\n",
    "successful_indices = [i for i, d in enumerate(data_list) if d is not None]\n",
    "data_list = [data_list[i] for i in successful_indices]\n",
    "labels = df['dili_concern'].iloc[successful_indices].values\n",
    "\n",
    "for i, data in enumerate(data_list):\n",
    "    data.y = torch.tensor([labels[i]], dtype=torch.float)\n",
    "\n",
    "print(f\"Successfully created {len(data_list)} graph objects.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "97c25039",
   "metadata": {},
   "source": [
    "### Create Train and Test Sets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b344d9ad",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "train_data, test_data = train_test_split(data_list, test_size=0.2, random_state=42, stratify=labels)\n",
    "train_loader = DataLoader(train_data, batch_size=32, shuffle=True)\n",
    "test_loader = DataLoader(test_data, batch_size=32, shuffle=False)\n",
    "\n",
    "print(f\"Number of training graphs: {len(train_data)}\")\n",
    "print(f\"Number of testing graphs: {len(test_data)}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e028857f",
   "metadata": {},
   "source": [
    "### Define the AttentiveFP Model\n",
    "\n",
    "Initialize the `AttentiveFP` model. We'll use parameters that are commonly effective for this architecture.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1bb37aae",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "# Determine feature sizes from our data\n",
    "num_node_features = data_list[0].x.shape[1]\n",
    "num_edge_features = data_list[0].edge_attr.shape[1]\n",
    "\n",
    "model = AttentiveFP(\n",
    "    in_channels=num_node_features,\n",
    "    hidden_channels=64,\n",
    "    out_channels=1, # Final output is a single value for binary classification\n",
    "    edge_dim=num_edge_features,\n",
    "    num_layers=2,\n",
    "    num_timesteps=2,\n",
    "    dropout=0.5\n",
    ")\n",
    "\n",
    "print(\"AttentiveFP Model defined:\")\n",
    "print(model)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "01e4789b",
   "metadata": {},
   "source": [
    "### Train the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6a00c9c",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "optimizer = torch.optim.Adam(model.parameters(), lr=0.001)\n",
    "scheduler = torch.optim.lr_scheduler.StepLR(optimizer, step_size=20, gamma=0.5)\n",
    "\n",
    "# Calculate class weights\n",
    "neg_count = np.sum(labels == 0)\n",
    "pos_count = np.sum(labels == 1)\n",
    "pos_weight_value = neg_count / pos_count if pos_count > 0 else 1\n",
    "pos_weight_tensor = torch.tensor([pos_weight_value], dtype=torch.float)\n",
    "criterion = torch.nn.BCEWithLogitsLoss(pos_weight=pos_weight_tensor)\n",
    "\n",
    "def train():\n",
    "    model.train()\n",
    "    total_loss = 0\n",
    "    for data in train_loader:\n",
    "        optimizer.zero_grad()\n",
    "        out = model(data.x, data.edge_index, data.batch, data.edge_attr)\n",
    "        loss = criterion(out, data.y.view(-1, 1))\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        total_loss += loss.item()\n",
    "    return total_loss / len(train_loader)\n",
    "\n",
    "print(\"Starting AttentiveFP training...\")\n",
    "for epoch in range(1, 101): # Train for 100 epochs\n",
    "    loss = train()\n",
    "    scheduler.step()\n",
    "    if epoch % 10 == 0:\n",
    "        print(f'Epoch: {epoch:02d}, Loss: {loss:.4f}')\n",
    "print(\"Training finished.\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "815afe22",
   "metadata": {},
   "source": [
    "### Evaluate the Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "244f5c84",
   "metadata": {
    "vscode": {
     "languageId": "plaintext"
    }
   },
   "outputs": [],
   "source": [
    "def test(loader):\n",
    "    model.eval()\n",
    "    all_preds = []\n",
    "    all_labels = []\n",
    "    with torch.no_grad():\n",
    "        for data in loader:\n",
    "            out = model(data.x, data.edge_index, data.batch, data.edge_attr)\n",
    "            preds = (torch.sigmoid(out) > 0.5).float()\n",
    "            all_preds.extend(preds.view(-1).tolist())\n",
    "            all_labels.extend(data.y.view(-1).tolist())\n",
    "    return np.array(all_preds), np.array(all_labels)\n",
    "\n",
    "y_pred, y_true = test(test_loader)\n",
    "\n",
    "# Calculate metrics\n",
    "afp_accuracy = accuracy_score(y_true, y_pred)\n",
    "afp_roc_auc = roc_auc_score(y_true, y_pred)\n",
    "\n",
    "\n",
    "# ## Step 7: Compare Results and Conclude\n",
    "\n",
    "# In[7]:\n",
    "print(\"--- AttentiveFP Model Performance ---\")\n",
    "print(f\"Accuracy: {afp_accuracy:.3f}\")\n",
    "print(f\"ROC AUC:  {afp_roc_auc:.3f}\")\n",
    "\n",
    "print(\"\\n--- Comparison ---\")\n",
    "print(\"Metric         | RandomForest (Baseline) | AttentiveFP Model\")\n",
    "print(\"----------------|-------------------------|-------------------\")\n",
    "rf_roc_auc = 0.761\n",
    "print(f\"ROC AUC       | {rf_roc_auc:.3f}                   | {afp_roc_auc:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "language_info": {
   "name": "python"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

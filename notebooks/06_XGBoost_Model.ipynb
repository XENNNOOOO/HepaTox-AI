{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b93df6c6",
   "metadata": {},
   "source": [
    "## Notebook 7: XGBoost Model Evaluation\n",
    "\n",
    "Train and evaluate an XGBoost classifier on our molecular fingerprint data.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4996bff4",
   "metadata": {},
   "source": [
    "### Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "16bf103e",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import numpy as np\n",
    "import xgboost as xgb\n",
    "from sklearn.model_selection import StratifiedKFold, cross_val_score, train_test_split\n",
    "from sklearn.metrics import accuracy_score, roc_auc_score\n",
    "import ast \n",
    "import matplotlib.pyplot as plt\n",
    "import seaborn as sns\n",
    "import optuna\n",
    "\n",
    "print(\"Libraries imported successfully.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f41381c8",
   "metadata": {},
   "source": [
    "### Load the Processed Data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9d282f87",
   "metadata": {},
   "outputs": [],
   "source": [
    "try:\n",
    "    df = pd.read_csv('data/processed/dili_data_clean.csv')\n",
    "    print(\"Processed data loaded successfully.\")\n",
    "    print(f\"Shape of the dataset: {df.shape}\")\n",
    "except FileNotFoundError:\n",
    "    print(\"Error: dili_data_clean.csv not found.\")\n",
    "    print(\"Please make sure you have uploaded the file to your Colab session.\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "95ffee20",
   "metadata": {},
   "source": [
    "### Prepare Data for Modeling"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "65aa6746",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Drop rows with missing fingerprints\n",
    "df.dropna(subset=['fingerprint'], inplace=True)\n",
    "\n",
    "# Convert the string representation of the list back into a list of integers\n",
    "df['fingerprint'] = df['fingerprint'].apply(ast.literal_eval)\n",
    "\n",
    "# Separate features (X) and target (y)\n",
    "X = np.array(df['fingerprint'].tolist())\n",
    "y = df['dili_concern'].values\n",
    "\n",
    "# Create a train/test split for final evaluation\n",
    "X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.2, random_state=42, stratify=y)\n",
    "\n",
    "\n",
    "print(f\"Data prepared. Training set shape: {X_train.shape}, Test set shape: {X_test.shape}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f4f20c",
   "metadata": {},
   "source": [
    "### Fine-Tune the XGBoost Model with Optuna\n",
    "\n",
    "Use Optuna to perform a more intelligent search for the best hyperparameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d32ef2c4",
   "metadata": {},
   "outputs": [],
   "source": [
    "def objective(trial):\n",
    "    # Define the search space for hyperparameters\n",
    "    param = {\n",
    "        'objective': 'binary:logistic',\n",
    "        'eval_metric': 'logloss',\n",
    "        'use_label_encoder': False,\n",
    "        'random_state': 42,\n",
    "        'n_estimators': trial.suggest_int('n_estimators', 50, 500),\n",
    "        'max_depth': trial.suggest_int('max_depth', 3, 10),\n",
    "        'learning_rate': trial.suggest_float('learning_rate', 0.01, 0.3, log=True),\n",
    "        'subsample': trial.suggest_float('subsample', 0.6, 1.0),\n",
    "        'colsample_bytree': trial.suggest_float('colsample_bytree', 0.6, 1.0),\n",
    "        'gamma': trial.suggest_float('gamma', 0, 0.5),\n",
    "        'min_child_weight': trial.suggest_int('min_child_weight', 1, 10),\n",
    "    }\n",
    "\n",
    "    # Add class weight balancing\n",
    "    neg_count = np.sum(y_train == 0)\n",
    "    pos_count = np.sum(y_train == 1)\n",
    "    param['scale_pos_weight'] = neg_count / pos_count if pos_count > 0 else 1\n",
    "\n",
    "    model = xgb.XGBClassifier(**param)\n",
    "    \n",
    "    # Perform cross-validation\n",
    "    cv = StratifiedKFold(n_splits=5, shuffle=True, random_state=42)\n",
    "    scores = cross_val_score(model, X_train, y_train, cv=cv, scoring='roc_auc', n_jobs=-1)\n",
    "    \n",
    "    return np.mean(scores)\n",
    "\n",
    "# Create an Optuna study and run the optimization\n",
    "# We want to maximize the ROC AUC score\n",
    "study = optuna.create_study(direction='maximize')\n",
    "print(\"Starting Optuna hyperparameter search...\")\n",
    "study.optimize(objective, n_trials=100) # Run for 100 trials\n",
    "\n",
    "print(\"\\nBest trial found:\")\n",
    "print(f\"  Value: {study.best_value:.4f}\")\n",
    "print(\"  Params: \")\n",
    "for key, value in study.best_params.items():\n",
    "    print(f\"    {key}: {value}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "67fa0ada",
   "metadata": {},
   "source": [
    "### Evaluate the Best Model"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12e954d9",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Get the best parameters\n",
    "best_params = study.best_params\n",
    "\n",
    "# Add the static parameters back in\n",
    "neg_count = np.sum(y_train == 0)\n",
    "pos_count = np.sum(y_train == 1)\n",
    "best_params['objective'] = 'binary:logistic'\n",
    "best_params['eval_metric'] = 'logloss'\n",
    "best_params['use_label_encoder'] = False\n",
    "best_params['random_state'] = 42\n",
    "best_params['scale_pos_weight'] = neg_count / pos_count if pos_count > 0 else 1\n",
    "\n",
    "# Train the final model with the best parameters\n",
    "best_model = xgb.XGBClassifier(**best_params)\n",
    "best_model.fit(X_train, y_train)\n",
    "\n",
    "# Make predictions on the test set\n",
    "y_pred = best_model.predict(X_test)\n",
    "y_pred_proba = best_model.predict_proba(X_test)[:, 1]\n",
    "\n",
    "# Calculate metrics\n",
    "tuned_accuracy = accuracy_score(y_test, y_pred)\n",
    "tuned_roc_auc = roc_auc_score(y_test, y_pred_proba)\n",
    "\n",
    "print(\"\\n--- Tuned XGBoost Model Performance ---\")\n",
    "print(f\"Accuracy on Test Set: {tuned_accuracy:.3f}\")\n",
    "print(f\"ROC AUC on Test Set:  {tuned_roc_auc:.3f}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cb02ff25",
   "metadata": {},
   "source": [
    "### Tuning Results Visualization"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2ec15fa",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot optimization history\n",
    "fig1 = plot_optimization_history(study)\n",
    "fig1.show()\n",
    "\n",
    "# Plot hyperparameter importances\n",
    "fig2 = plot_param_importances(study)\n",
    "fig2.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "809667eb",
   "metadata": {},
   "source": [
    "### Final Comparison"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02fc3565",
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"\\n--- Comparison ---\")\n",
    "print(\"Metric         | RandomForest (Baseline) | Tuned XGBoost Model\")\n",
    "print(\"----------------|-------------------------|---------------------\")\n",
    "rf_roc_auc = 0.761 # From our previous best model\n",
    "print(f\"ROC AUC       | {rf_roc_auc:.3f}                   | {tuned_roc_auc:.3f}\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
